{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globus data publication platform tutorial\n",
    "\n",
    "In this tutorial we will demonstrate how the Globus platform can be used to create flexible publication pipelines that can be used to publish arbitrary data, with flexible access control, descriptive metadata, and persistent identifiers. \n",
    "\n",
    "We will walk through an example data publication workflow as follows. First we will move data to a remote endpoint, make that data immutable, and share it with those who can access it; we then use the Globus Identifier service to mint a persistent identifier for the data; we then index descriptive metadata in Globus Search such that is discoverable by other users; finally, we demonstrate an example web portal for discovering and accessing the published datasets \n",
    "\n",
    "The basic tutorial flow is illustrated below.  \n",
    "\n",
    "![title](publication-flow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function  # for python 2\n",
    "import globus_sdk\n",
    "from identifier_api import IdentifierClient\n",
    "import json\n",
    "\n",
    "# Globus Endpoint for storing publication (Petrel#Testbed)\n",
    "publication_endpoint = \"e56c36e4-1063-11e6-a747-22000bf2d559\"\n",
    "\n",
    "# Globus Group which can view publications\n",
    "access_group = \"50b6a29c-63ac-11e4-8062-22000ab68755\"\n",
    "\n",
    "# HTTPS URL for the publication endpoint\n",
    "http_base_url = \"https://testbed.petrel.host/\"\n",
    "\n",
    "# Search index ID to store metadata\n",
    "search_index = \"3e117028-2513-4f5b-b53c-90fda3cd328b\"\n",
    "\n",
    "# ID of namespace where we create identifiers\n",
    "identifiers_namespace = \"2lxaZcq_7D4j\"\n",
    "\n",
    "# ID of this tutorial notebook as a Globus App\n",
    "CLIENT_ID = 'd61ed2e0-b4f9-4fe9-9433-41e2528a807d'\n",
    "\n",
    "# python2/3 safe simple input reading\n",
    "get_input = getattr(__builtins__, 'raw_input', input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Authentication\n",
    "\n",
    "Boefore implementing the publication workflow we must first authenticate with Globus and request access tokens to access the transfer, search, and identifier services. \n",
    "\n",
    "We follow a standard OAuth 2 authenticaiton flow for native applications. The first step is to create a unique link via which a user can authenticate. We then capture the resulting auth code as input in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a client which is responsible for managing interactions with the Globus Auth service\n",
    "# it manages the entire OAuth2 login flow\n",
    "native_auth_client = globus_sdk.NativeAppAuthClient(CLIENT_ID)\n",
    "\n",
    "# start a flow with a specific set of requested scopes -- levels of access to Globus Apps & Services\n",
    "# When you login, you will be prompted to accept this App's access to these services\n",
    "transfer_scope = 'urn:globus:auth:scope:transfer.api.globus.org:all'\n",
    "search_scope = 'urn:globus:auth:scope:search.api.globus.org:all'\n",
    "identifiers_scope = 'https://auth.globus.org/scopes/identifiers.globus.org/create_update'\n",
    "native_auth_client.oauth2_start_flow(\n",
    "    requested_scopes=[\n",
    "        transfer_scope,\n",
    "        search_scope,\n",
    "        identifiers_scope\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Login link for an authorization code\n",
    "# this is like a one-time password used to fetch longer-lasting credentials (tokens)\n",
    "print(\"Login Here:\\n\\n{0}\".format(native_auth_client.oauth2_get_authorize_url()))\n",
    "print((\"\\n\\nNote that this link can only be used once! \"\n",
    "       \"If login or a later step in the flow fails, you must restart it.\"))\n",
    "\n",
    "# fill this line in with the code that you got\n",
    "auth_code = get_input(\"Enter resulting code:\")\n",
    "\n",
    "# and exchange it for a response object containing your token(s)\n",
    "# we'll use this \"tokens\" object in later steps\n",
    "tokens = native_auth_client.oauth2_exchange_code_for_tokens(auth_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second phase of the OAuth2 flow we use the access code to obtain access tokens for each service and create Python clients for each service using the Globus SDK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the Access Tokens for Globus Transfer, Globus Search, and Globus Identifiers\n",
    "#\n",
    "# for full detail, see the SDK documentation:\n",
    "# http://globus-sdk-python.readthedocs.io/en/stable/responses/auth/#globus_sdk.auth.token_response.OAuthTokenResponse\n",
    "transfer_tokdata = tokens.by_scopes[transfer_scope]\n",
    "search_tokdata = tokens.by_scopes[search_scope]\n",
    "identifiers_tokdata = tokens.by_scopes[identifiers_scope]\n",
    "\n",
    "# pull out Access Tokens -- 48-hour credentials which can be used to access the services\n",
    "transfer_token = transfer_tokdata['access_token']\n",
    "search_token = search_tokdata['access_token']\n",
    "identifiers_token = identifiers_tokdata['access_token']\n",
    "\n",
    "# to pass tokens to clients, wrap them in GlobusAuthorizers\n",
    "# these are generic objects which support multiple authentication methods -- Access Tokens are just one\n",
    "# and pass the results to client objects\n",
    "transfer = globus_sdk.TransferClient(\n",
    "    authorizer=globus_sdk.AccessTokenAuthorizer(transfer_token))\n",
    "search = globus_sdk.SearchClient(\n",
    "    authorizer=globus_sdk.AccessTokenAuthorizer(search_token))\n",
    "identifiers = IdentifierClient(\n",
    "    authorizer=globus_sdk.AccessTokenAuthorizer(identifiers_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Assemble a dataset\n",
    "\n",
    "In the first stage of the publication workflow we move the data to be published to a location that is immuatable, accessible to those who might wish to access the data, and can scale to the required data size. For this purpose we use a Globus shared endpoint as it allows us to dynamically manage access to data. \n",
    "\n",
    "To isloate users' publications from each other we create a unique directory for our publication on our shared endpoint. To avoid conflcits we will name the directory using a UUID.\n",
    "\n",
    "Note: to follow these instructions you will need to make sure you are in the [Tutorial Users Group](https://www.globus.org/app/groups/50b6a29c-63ac-11e4-8062-22000ab68755)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# create a unqiue id for the directory name\n",
    "share_path = '/' + str(uuid.uuid4()) + '/'\n",
    "r = transfer.operation_mkdir(publication_endpoint, path=share_path)\n",
    "\n",
    "print(\"Publication path: %s\" % share_path)\n",
    "print(\"https://www.globus.org/app/transfer?origin_id=%s&origin_path=%s\" % (publication_endpoint, share_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having created the directory we now need to populate it with our dataset. For simplicity, we will move sample Globus data from the \"Globus Tutorial Endpoint.\" You are welcome to use any data you like, just update the `source_endpoint` and source_directory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the source endpoint and directory to be copied for publication\n",
    "# (Globus Tutorial Endpoint 1):/share/godata/\n",
    "source_endpoint = 'ddb59aef-6d04-11e5-ba46-22000b92c6ec'\n",
    "source_directory= '/share/godata/'\n",
    "\n",
    "# TransferData is a helper function for building good Transfer Task documents for the Globus Transfer Service\n",
    "tdata = globus_sdk.TransferData(\n",
    "    transfer, source_endpoint, publication_endpoint,\n",
    "    label='Tutorial copy data', sync_level='checksum')\n",
    "\n",
    "# you can add multiple files and directories to transfer -- for our case, just add one\n",
    "tdata.add_item(source_directory, share_path, recursive=True)\n",
    "\n",
    "# once it's built, submit the transfer and get a task document to describe it\n",
    "task_description = transfer.submit_transfer(tdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now wait for the transfer to complete using the Globus SDK `task_wait` function. To confirm that the data is transferred correctly we preform an `ls` operation on the shared endpoint. Note: in this example we also record the last file name in the publication directory so that we can associate metadata later in the tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: It's technically possible for the task to terminate with a failure. This code does not handle this condition.\n",
    "\n",
    "# wait up to 100s, checking every 1s\n",
    "completed = transfer.task_wait(\n",
    "    task_description['task_id'], timeout=100, polling_interval=1)\n",
    "\n",
    "share_file = None\n",
    "\n",
    "if not completed:\n",
    "    print('Transfer still not completed!')\n",
    "else:\n",
    "    for f in transfer.operation_ls(publication_endpoint, path=share_path):\n",
    "        print(f['name'])\n",
    "        share_file = f['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is placed on a shared endpoint, and in a unique publication directory, we can share the published data with individuals or groups of users. Below we share the published data with the \"Tutorial Users Group\" so that other tutorial participants will be able to view and download your published data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a rule which\n",
    "# - allows Read access, permissions=\"r\"\n",
    "# - on the directory we generated above, share_path\n",
    "# - for the Tutorial Users Group, access_group\n",
    "rule_data = {\n",
    "    'DATA_TYPE': 'access',\n",
    "    'principal_type': 'group', \n",
    "    'principal': access_group,\n",
    "    'path': share_path,\n",
    "    'permissions': 'r'\n",
    "}\n",
    "\n",
    "result = transfer.add_endpoint_acl_rule(publication_endpoint, rule_data)\n",
    "print(result['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Create Metadata to describe our dataset\n",
    "\n",
    "We will define simple metadata which describes our dataset. This metadata will be used for registering the identifier and also for loading into our search index to enable discovery of the published dataset.\n",
    "\n",
    "You should update the metadata below to reflect your publication. Add your name as a contributor and update the title, year and keywords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    'title': 'My Publication for GW18',\n",
    "    'contributors': ['John Smith', 'Jane Doe', 'Zaphod Beeblebrox'],\n",
    "    'date': '2018-12-12',\n",
    "    'keywords': ['Hitchhiker', 'Blanket', 'Panic']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  4) Associate an Identifier\n",
    "\n",
    "The next stage of our workflow is to associate an persistent and unambiguous identifier with the dataset. This is advantageous as it allows others to refer to a permanant name rather than a potentially volatile reference to a storage location. \n",
    "\n",
    "The Globus Identifier service allows users to create identifiers within user-managed namespaces. Namespaces abstract use of an external persistent identifier (PID) provider and a valid account (or shoulder) within that provider. \n",
    "\n",
    "When minting an identifier in a namespace the following information must be provided: 1) one or more locations to access the data such as a URL representing a particular path on a Globus endpoint; 2) metadata describing a mixture of publication-specific attributes (e.g., creator, checksum) and optionally extensible, user-defined attributes; 3) access policies governing which users can access the identifier. \n",
    "\n",
    "First, we'll introspect the namespace to confirm it is the correct namespace for our publication. The result of introspection includes the administrators of the namespace, creators who are able to mint identifiers as well as various metadata fields that describe the namespace (e.g., name and description). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_namespace = identifiers.get_namespace(identifiers_namespace)\n",
    "\n",
    "print(\"Name: %s\" % identifier_namespace.data['display_name'])\n",
    "print(\"Description: %s\" % identifier_namespace.data['description'])\n",
    "print(\"Provider: %s\" % identifier_namespace.data['provider_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create an identifier in the namespace. We set the location for the data to a Globus URI that includes the endpoint and folder where we stored the data. We also associate basic metadata about the dataset. Finally, we will set the identifier to be visible to all users ('public').\n",
    "\n",
    "The Identifier service returns a JSON description of the identifier including the metadata we defined above and the newly minted identifier.  In this case our namespace is configured to create an ARK using the test shoulder. \n",
    "\n",
    "We also add the newly minted identifier to the dataset's metadata so that we can load it into our search index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a location for accessing the data\n",
    "dataset_location = \"globus://%s%s\" % (publication_endpoint, share_path)\n",
    "visible_to = ['public']\n",
    "dataset_identifier = identifiers.create_identifier(\n",
    "    namespace=identifiers_namespace,\n",
    "    location=[dataset_location],\n",
    "    properties={\n",
    "        'title': metadata['title'],\n",
    "        'date': metadata['date'],\n",
    "        'contributors': metadata['contributors']\n",
    "    },\n",
    "    visible_to=visible_to)\n",
    "\n",
    "metadata['identifier'] = dataset_identifier.data['identifier']\n",
    "\n",
    "print(\"Identifier %s\" % dataset_identifier.data['identifier'])\n",
    "print(\"location %s\" % dataset_identifier.data['location'])\n",
    "print(\"Metadata %s\" % dataset_identifier.data['properties'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have minted the identifier we can resolve it to find out metadata and retrieve a link to the data. For this purpose we use an online resolver: the name 2 thing resolver (n2t.net). \n",
    "\n",
    "Note: registration takes a few moments to propogate. If the identifier doesn't resolve, please wait a few seconds and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('https://n2t.net/{}'.format(metadata['identifier']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Index descriptive metadata\n",
    "\n",
    "In the third stage of the workflow we aim to index the metadata that describes our published dataset. For this purpose we use Globus Search: a flexible, schema-agnostic search platform with fine grain access control on published records and metadata. Globus Search provides powerful free-text search capabilities via which others can discover our published dataset.\n",
    "\n",
    "Globus Search supports user-managed indexes in which an adminstrator may create an index and define policies regarding its use including who can manage the index, ingest metadata, and query the index. \n",
    "\n",
    "Complete documentation for using Globus Search is available online: https://docs.globus.org/api/search/\n",
    "\n",
    "We have created an index for this tutorial. You can use the Globus SDK to retrieve information about the index as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial_index = search.get_index(index_id=search_index)\n",
    "print(tutorial_index['display_name'])\n",
    "print(tutorial_index['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Data\n",
    "\n",
    "Globus Search supports scalable indexing of arbitrary entries into a selected index.  An entry is comprised of three types of information: 1) a subject, which represents a name or target for the entry (e.g., a URL for a Globus-accesible file or directory); 2) arbitrary metadata represented as a collection of attributes in nested JSON structure; and 3) a visibility policy that defines which users or groups are able to view and query the subject and its metadata.\n",
    "\n",
    "To index metadata we construct an JSON object which includes this information as follows and use the `ingest` function to add it to the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject =  \"globus://%s%s%s\" % (publication_endpoint, share_path, share_file)\n",
    "ingest_data = {\n",
    "    \"ingest_type\": \"GMetaEntry\",\n",
    "    \"ingest_data\": {\n",
    "        \"subject\": subject,\n",
    "        \"visible_to\": [\"public\"],\n",
    "        \"content\": metadata\n",
    "    }\n",
    "}\n",
    "result = search.ingest(search_index, ingest_data)\n",
    "print(\"Documents indexed: %s\" % result['num_documents_ingested'])\n",
    "print(\"Subject: %s\" % subject)\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "Globus Search implements a flexible query model that supports two types of queries: simple, free-text queries and complex, structured queries.\n",
    "\n",
    "Simple queries perform basic sub-string matching against any metadata fields that are visible to the querying user.\n",
    "As with web search, the results of a simple search are ordered based on the computed \"best match\" for the query. \n",
    "\n",
    "A simple query is as easy as passing a string to the `search` function.  The results are an ordered list of result objects. \n",
    "\n",
    "Update the following free text query to discover your dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='john'\n",
    "\n",
    "search_results = search.search(index_id=search_index, q=query)\n",
    "\n",
    "print(\"Count: %s\" % search_results['count'])\n",
    "for i in search_results['gmeta']:\n",
    "    print(\"Subject: %s\" % i['subject'])\n",
    "    print(\"Content: %s\" % json.dumps(i['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Globus Search also supports an advanced query mode in which more precise queries can be expressed. For examples, queries that search specific attributes, range expressions, exact matches, and so forth.\n",
    "\n",
    "First we search for your published dataset using the minted identiifer, we then query for all publications with a specific contributor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = search.search(search_index, q='identifier: \"%s\"' % metadata['identifier'], advanced=True)\n",
    "\n",
    "print(\"Count: %s\" % search_results['count'])\n",
    "for i in search_results['gmeta']:\n",
    "    print(\"Subject: %s\" % i['subject'])\n",
    "    print(\"Content: %s\" % i['content'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = search.search(search_index, 'contributors: \"John Smith\"', advanced=True)\n",
    "\n",
    "print(\"Count: %s\" % search_results['count'])\n",
    "for i in search_results['gmeta']:\n",
    "    print(\"Subject: %s\" % i['subject'])\n",
    "    print(\"Content: %s\" % json.dumps(i['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex queries\n",
    "\n",
    "Complex queries, in contrast, take the form of a structured JSON document, and are more commonly used when queries are created programmatically. They may reference specific metadata fields, and may apply criteria such as value ranges, wildcards, and\n",
    "regular expressions. \n",
    "\n",
    "For example, to conduct the same free-text search as above but to limit results to publications between 2010-2020 we can add a filter to the query.\n",
    "\n",
    "Note: we use the Globus SDK SearchQuery to construct complex queries.  We also show the resulting JSON query object used to execute the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_query = (globus_sdk.SearchQuery(q=query)\n",
    "                    .add_filter('year', [{'from': 2000, 'to': 2020}], type='range'))\n",
    "search_result = search.post_search(search_index, structured_query)\n",
    "\n",
    "print(\"Structured Query Object: %s\\n\" % json.dumps(structured_query))\n",
    "print(\"Count: %s\" % search_results['count'])\n",
    "for i in search_results['gmeta']:\n",
    "    print(\"Subject: %s\" % i['subject'])\n",
    "    print(\"Content: %s\\n\" % json.dumps(i['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complex queries may also specify facets---a method for generating categories and associated frequencies for particular metadata fields.  For example, here is a query to produce keyword facets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "structured_query = (globus_sdk.SearchQuery(q='*').add_facet('Publication Keywords', 'keywords'))\n",
    "search_results = search.post_search(search_index, structured_query)\n",
    "\n",
    "print(\"Structured Query Object: %s\\n\" % json.dumps(structured_query))\n",
    "\n",
    "print(\"Results\\nCount: %s\" % search_results['count'])\n",
    "#for i in search_results['gmeta']:\n",
    "#    print(\"Subject: %s\" % i['subject'])\n",
    "#    print(\"Content: %s\" % json.dumps(i['content']))\n",
    "\n",
    "print(\"\\nFacets\")\n",
    "for i in search_results['facet_results']:\n",
    "    for j in i['buckets']:\n",
    "        print (\"%s (%s)\" % (j['value'], j['count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced indexing\n",
    "\n",
    "One of the benefits of the Globus Search model is that you can associate visibility policies to records and metadata. Here we demonstrate how you can add a new metadata entry to a record and make it visible only to a particular group of users. \n",
    "\n",
    "Update the metadata added below and confirm that the queries now show the updated metadata. Note: when querying over these entities the results will collapse metadata for the same root subject. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_data = {\n",
    "    \"ingest_type\": \"GMetaEntry\",\n",
    "    \"ingest_data\": {\n",
    "        \"subject\": \"globus://%s%s%s\" % (publication_endpoint, share_path, share_file),\n",
    "        \"id\": \"rating\",\n",
    "        \"visible_to\": ['urn:globus:groups:id:%s' % access_group],\n",
    "        \"content\": {\n",
    "            \"rating\": \"good\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "result = search.ingest(search_index, ingest_data)\n",
    "print(\"Documents indexed: %s\" % result['num_documents_ingested'])\n",
    "\n",
    "search_results = search.search(search_index, q='identifier: \"%s\"' % metadata['identifier'], advanced=True)\n",
    "\n",
    "print(\"Count: %s\" % search_results['count'])\n",
    "for i in search_results['gmeta']:\n",
    "    print(\"Subject: %s\" % i['subject'])\n",
    "    print(\"Content: %s\" % i['content'])                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Browse published datasets\n",
    "\n",
    "Finally, we can build powerful user interfaces on top of Globus publication platform services. As one example we have developed a simple Django portal for browsing and searching publised datasets. \n",
    "\n",
    "The portal is available here: https://portal.demo.globus.org\n",
    "\n",
    "Try the same queries as above to find your indexed data. The datasets you ingested above will be immediately avaialble in the search portal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Miniconda3]",
   "language": "python",
   "name": "conda-env-Miniconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
