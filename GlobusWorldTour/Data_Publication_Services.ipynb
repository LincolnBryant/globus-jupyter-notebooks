{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globus data publication platform tutorial\n",
    "\n",
    "In this tutorial we will demonstrate how the Globus platform can be used to create automated pipelines that can be used to publish arbitrary data, with flexible access control, descriptive metadata, and persistent identifiers. \n",
    "\n",
    "We will walk through the following data publication flow:\n",
    "1. Authenticate with Globus and get tokens for accessing various services\n",
    "1. Assemble a dataset and move the data to a remote, immutable endpoint, with restricted access\n",
    "1. Define some metadata for our dataset\n",
    "1. Use the Globus Identifier service to mint a persistent identifier for the data\n",
    "1. Index descriptive metadata in Globus Search such that is discoverable by other users\n",
    "\n",
    "The basic tutorial flow is illustrated below.  \n",
    "\n",
    "<img src=\"img/publication_flow.png\" alt=\"Automated data publication flow\" align=\"CENTER\" style=\"width: 85%;\"/>\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "To complete this tutorial you will need to make sure you are in the [Tutorial Users Group](https://www.globus.org/app/groups/50b6a29c-63ac-11e4-8062-22000ab68755)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function  # for python 2\n",
    "import globus_sdk\n",
    "from identifier_api import IdentifierClient\n",
    "import json\n",
    "\n",
    "# Globus Endpoint for storing publication (Petrel#Testbed)\n",
    "publication_endpoint = \"e56c36e4-1063-11e6-a747-22000bf2d559\"\n",
    "\n",
    "# Globus Group which can view publications\n",
    "access_group = \"50b6a29c-63ac-11e4-8062-22000ab68755\"\n",
    "\n",
    "# HTTPS URL for the publication endpoint\n",
    "http_base_url = \"https://testbed.petrel.host/\"\n",
    "\n",
    "# Search index ID to store metadata\n",
    "search_index = \"f702761b-3a05-4ba1-af2b-c0e07850c6f1\"\n",
    "\n",
    "# ID of namespace where we create identifiers\n",
    "identifiers_namespace = \"HHxPIZaVDh9u\"\n",
    "\n",
    "# python2/3 safe simple input reading\n",
    "get_input = getattr(__builtins__, 'raw_input', input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Authentication\n",
    "\n",
    "Boefore implementing the automated publication flow we must authenticate with Globus and request access tokens to use the transfer, search, and identifier services. Here we get the tokens avaialable in JupyterHub, and create clients for interacting with Globus services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, base64, os, pprint\n",
    "\n",
    "data = pickle.loads(base64.b64decode(os.getenv('GLOBUS_DATA')))\n",
    "\n",
    "transfer_token = data['tokens']['transfer.api.globus.org']['access_token']\n",
    "search_token = data['tokens']['search.api.globus.org']['access_token']\n",
    "identifiers_token = data['tokens']['identifiers.globus.org']['access_token']\n",
    "\n",
    "# to pass tokens to clients, wrap them in GlobusAuthorizers\n",
    "# these are generic objects which support multiple authentication methods -- Access Tokens are just one\n",
    "# and pass the results to client objects\n",
    "transfer = globus_sdk.TransferClient(\n",
    "    authorizer=globus_sdk.AccessTokenAuthorizer(transfer_token))\n",
    "search = globus_sdk.SearchClient(\n",
    "    authorizer=globus_sdk.AccessTokenAuthorizer(search_token))\n",
    "identifiers = IdentifierClient(\n",
    "    authorizer=globus_sdk.AccessTokenAuthorizer(identifiers_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Assemble a dataset\n",
    "\n",
    "In the first stage of the publication workflow we move the data to a location that is immuatable, accessible only to authorized users (i.e. those in the Tutorial Users group), and able to scale as needed. We use a Globus shared endpoint for this purpose, as it allows us to dynamically manage access to data. \n",
    "\n",
    "To isloate users' datasets from each other we create a unique directory on our shared endpoint (to avoid conflcits we will name the directory using a UUID).\n",
    "\n",
    "Note: to follow these instructions you will need to make sure you are in the [Tutorial Users Group](https://www.globus.org/app/groups/50b6a29c-63ac-11e4-8062-22000ab68755)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# create a unqiue id for the directory name\n",
    "share_path = '/' + str(uuid.uuid4()) + '/'\n",
    "r = transfer.operation_mkdir(publication_endpoint, path=share_path)\n",
    "\n",
    "print(\"Publication path: %s\" % share_path)\n",
    "print(\"https://www.globus.org/app/transfer?origin_id=%s&origin_path=%s\" % (publication_endpoint, share_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having created the directory we now need to populate it with our dataset. For simplicity, we will move sample Globus data from the \"Globus Tutorial Endpoint.\" You are welcome to use any data you like, just update the `source_endpoint` and source_directory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the source endpoint and directory to be copied for publication\n",
    "# (Globus Tutorial Endpoint 1):/share/godata/\n",
    "source_endpoint = 'ddb59aef-6d04-11e5-ba46-22000b92c6ec'\n",
    "source_directory= '/share/godata/'\n",
    "\n",
    "# TransferData is a helper function for building good Transfer Task documents for the Globus Transfer Service\n",
    "tdata = globus_sdk.TransferData(\n",
    "    transfer, source_endpoint, publication_endpoint,\n",
    "    label='Tutorial copy data', sync_level='checksum')\n",
    "\n",
    "# you can add multiple files and directories to transfer -- for our case, just add one\n",
    "tdata.add_item(source_directory, share_path, recursive=True)\n",
    "\n",
    "# once it's built, submit the transfer and get a task document to describe it\n",
    "task_description = transfer.submit_transfer(tdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now wait for the transfer to complete using the Globus SDK `task_wait` function. To confirm that the data is transferred correctly we preform an `ls` operation on the shared endpoint. Note: in this example we also record the last file name in the publication directory so that we can associate metadata later in the tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: It's technically possible for the task to terminate with a failure. This code does not handle this condition.\n",
    "\n",
    "# wait up to 100s, checking every 1s\n",
    "completed = transfer.task_wait(\n",
    "    task_description['task_id'], timeout=100, polling_interval=1)\n",
    "\n",
    "share_file = None\n",
    "\n",
    "if not completed:\n",
    "    print('Transfer still not completed!')\n",
    "else:\n",
    "    for f in transfer.operation_ls(publication_endpoint, path=share_path):\n",
    "        print(f['name'])\n",
    "        share_file = f['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is placed on a shared endpoint, and in a unique publication directory, we can share the published data with individuals or groups of users. Below we share the published data with the \"Tutorial Users Group\" so that other tutorial participants will be able to view and download your published data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a rule which\n",
    "# - allows Read access, permissions=\"r\"\n",
    "# - on the directory we generated above, share_path\n",
    "# - for the Tutorial Users Group, access_group\n",
    "rule_data = {\n",
    "    'DATA_TYPE': 'access',\n",
    "    'principal_type': 'group', \n",
    "    'principal': access_group,\n",
    "    'path': share_path,\n",
    "    'permissions': 'r'\n",
    "}\n",
    "\n",
    "result = transfer.add_endpoint_acl_rule(publication_endpoint, rule_data)\n",
    "print(result['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create Metadata to describe our dataset\n",
    "\n",
    "We will define simple metadata which describes our dataset. This metadata will be used for registering the identifier and also for loading into our search index to enable discovery of the published dataset.\n",
    "\n",
    "You should update the metadata below to reflect your publication. Add your name as a contributor and update the title, date, and keywords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    'title': 'My GlobusWorld Tour Publication',\n",
    "    'contributors': ['John Smith', 'Jane Doe', 'Zaphod Beeblebrox'],\n",
    "    'date': '2018-12-12',\n",
    "    'keywords': ['Hitchhiker', 'Blanket', 'Panic']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  4. Associate an Identifier\n",
    "\n",
    "Next we associate a persistent and unambiguous identifier with the dataset. This allows others to refer to a permanent name rather than a potentially volatile storage location reference. \n",
    "\n",
    "The Globus Identifier service allows users to create identifiers within user-managed namespaces. Namespaces abstract use of an external persistent identifier (PID) provider and a valid account (or shoulder) within that provider. \n",
    "\n",
    "When minting an identifier the following information must be provided:\n",
    "* One or more locations to access the data, such as a URL representing a particular path on a Globus endpoint\n",
    "* Metadata describing a mixture of publication-specific attributes (e.g., creator, checksum) and optionally extensible, user-defined attributes\n",
    "* Access policies governing which users can access the identifier\n",
    "\n",
    "First, we'll introspect the namespace to confirm it is the correct namespace for our publication. The result of introspection includes the administrators of the namespace, creators who are able to mint identifiers as well as various metadata fields that describe the namespace (e.g., name and description). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_namespace = identifiers.get_namespace(identifiers_namespace)\n",
    "\n",
    "print(\"Name: %s\" % identifier_namespace.data['display_name'])\n",
    "print(\"Description: %s\" % identifier_namespace.data['description'])\n",
    "print(\"Provider: %s\" % identifier_namespace.data['provider_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create an identifier in the namespace. We set the location for the data to a Globus URI that includes the endpoint and folder where we stored the data. We also associate basic metadata about the dataset. Finally, we will set the identifier to be visible to all users ('public').\n",
    "\n",
    "The Identifier service returns a JSON description of the identifier, including the metadata we defined above and the newly minted identifier. In this case our namespace is configured to create an ARK using the test shoulder. \n",
    "\n",
    "We also add the newly minted identifier to the dataset's metadata so that we can load it into our search index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'publication_endpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dd90baa6fb7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# define a location for accessing the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"globus://%s%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpublication_endpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshare_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvisible_to\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'public'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m dataset_identifier = identifiers.create_identifier(\n\u001b[1;32m      5\u001b[0m     \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midentifiers_namespace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'publication_endpoint' is not defined"
     ]
    }
   ],
   "source": [
    "# define a location for accessing the data\n",
    "dataset_location = \"globus://%s%s\" % (publication_endpoint, share_path)\n",
    "visible_to = ['public']\n",
    "dataset_identifier = identifiers.create_identifier(\n",
    "    namespace=identifiers_namespace,\n",
    "    location=[dataset_location],\n",
    "    metadata={\n",
    "        'title': metadata['title'],\n",
    "        'date': metadata['date'],\n",
    "        'contributors': metadata['contributors']\n",
    "    },\n",
    "    visible_to=visible_to)\n",
    "\n",
    "metadata['identifier'] = dataset_identifier.data['identifier']\n",
    "\n",
    "print(\"Identifier %s\" % dataset_identifier.data['identifier'])\n",
    "print(\"location %s\" % dataset_identifier.data['location'])\n",
    "print(\"Metadata %s\" % dataset_identifier.data['metadata'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have minted the identifier we can resolve it to find out metadata and retrieve a link to the data. For this purpose we use an online resolver: the name 2 thing resolver (n2t.net). \n",
    "\n",
    "Note: registration takes a few moments to propogate. If the identifier doesn't resolve, please wait a few seconds and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('https://n2t.net/{}'.format(metadata['identifier']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Index descriptive metadata\n",
    "\n",
    "In this stage of the flow we aim to index the metadata that describes our published dataset. For this purpose we use Globus Search, a flexible, schema-agnostic search platform with fine grained access control on data and metadata. Globus Search provides powerful, free-text search capabilities via which others can discover our published dataset.\n",
    "\n",
    "Globus Search supports user-managed indexes in which an adminstrator may create an index and define policies regarding its use, including who can manage the index, ingest metadata, and query the index. \n",
    "\n",
    "Complete documentation for using Globus Search is available at https://docs.globus.org/api/search/.\n",
    "\n",
    "We have created an index for this tutorial. You can use the Globus SDK to retrieve information about the index as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial_index = search.get_index(index_id=search_index)\n",
    "print(tutorial_index['display_name'])\n",
    "print(tutorial_index['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Data\n",
    "\n",
    "Globus Search supports scalable indexing of arbitrary entries into a selected index. An entry is comprised of three types of information:\n",
    "1. A subject, which represents a name or target for the entry (e.g., a URL for a Globus-accesible file or directory)\n",
    "1. Arbitrary metadata represented as a collection of attributes in nested JSON structure\n",
    "1. A visibility policy that defines which users or groups are able to view and query the subject and its metadata\n",
    "\n",
    "To index metadata we construct an JSON object that includes this information, and use the `ingest` function to add it to the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject =  \"globus://%s%s%s\" % (publication_endpoint, share_path, share_file)\n",
    "ingest_data = {\n",
    "    \"ingest_type\": \"GMetaEntry\",\n",
    "    \"ingest_data\": {\n",
    "        \"subject\": subject,\n",
    "        \"visible_to\": [\"public\"],\n",
    "        \"content\": metadata\n",
    "    }\n",
    "}\n",
    "result = search.ingest(search_index, ingest_data)\n",
    "print(\"Documents indexed: %s\" % result['num_documents_ingested'])\n",
    "print(\"Subject: %s\" % subject)\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "Globus Search implements a flexible query model that supports two types of queries: simple, free-text queries and complex, structured queries.\n",
    "\n",
    "Simple queries perform basic sub-string matching against any metadata fields that are visible to the querying user.\n",
    "As with web search, the results of a simple search are ordered based on the computed \"best match\" for the query. \n",
    "\n",
    "A simple query is as easy as passing a string to the `search` function.  The results are an ordered list of result objects. \n",
    "\n",
    "Update the following free text query to discover your dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='john'\n",
    "\n",
    "search_results = search.search(index_id=search_index, q=query)\n",
    "\n",
    "print(\"Count: %s\" % search_results['count'])\n",
    "for i in search_results['gmeta']:\n",
    "    print(\"Subject: %s\" % i['subject'])\n",
    "    print(\"Content: %s\" % json.dumps(i['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Globus Search also supports an advanced query mode in which more precise queries can be expressed. For examples, queries that search specific attributes, range expressions, exact matches, and so forth.\n",
    "\n",
    "First we search for your published dataset using the minted identiifer, we then query for all publications with a specific contributor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = search.search(search_index, q='identifier: \"%s\"' % metadata['identifier'], advanced=True)\n",
    "\n",
    "print(\"Count: %s\" % search_results['count'])\n",
    "for i in search_results['gmeta']:\n",
    "    print(\"Subject: %s\" % i['subject'])\n",
    "    print(\"Content: %s\" % i['content'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = search.search(search_index, 'contributors: \"John Smith\"', advanced=True)\n",
    "\n",
    "print(\"Count: %s\" % search_results['count'])\n",
    "for i in search_results['gmeta']:\n",
    "    print(\"Subject: %s\" % i['subject'])\n",
    "    print(\"Content: %s\" % json.dumps(i['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex queries\n",
    "\n",
    "Complex queries take the form of a structured JSON document, and are more commonly used when the queries is created programmatically. They may reference specific metadata fields, and may apply criteria such as value ranges, wildcards, and regular expressions. \n",
    "\n",
    "For example, to conduct the same free-text search as above&mdash;but to limit results to publications between 2010-2020&mdash;we can add a filter to the query.\n",
    "\n",
    "Note: We use the Globus SDK SearchQuery to construct complex queries. We also show the resulting JSON query object used to execute the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_query = (globus_sdk.SearchQuery(q=query)\n",
    "                    .add_filter('date', [{'from': 2000, 'to': 2020}], type='range'))\n",
    "search_results = search.post_search(search_index, structured_query)\n",
    "\n",
    "print(\"Structured Query Object: %s\\n\" % json.dumps(structured_query))\n",
    "print(\"Count: %s\" % search_results['count'])\n",
    "for i in search_results['gmeta']:\n",
    "    print(\"Subject: %s\" % i['subject'])\n",
    "    print(\"Content: %s\\n\" % json.dumps(i['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complex queries may also specify facets&mdash;a method for generating categories and associated frequencies for particular metadata fields. For example, here is a query to produce keyword facets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "structured_query = (globus_sdk.SearchQuery(q='*').add_facet('Publication Keywords', 'keywords'))\n",
    "search_results = search.post_search(search_index, structured_query)\n",
    "\n",
    "print(\"Structured Query Object: %s\\n\" % json.dumps(structured_query))\n",
    "\n",
    "print(\"Results\\nCount: %s\" % search_results['count'])\n",
    "#for i in search_results['gmeta']:\n",
    "#    print(\"Subject: %s\" % i['subject'])\n",
    "#    print(\"Content: %s\" % json.dumps(i['content']))\n",
    "\n",
    "print(\"\\nFacets\")\n",
    "for i in search_results['facet_results']:\n",
    "    for j in i['buckets']:\n",
    "        print (\"%s (%s)\" % (j['value'], j['count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced indexing\n",
    "\n",
    "One of the benefits of the Globus Search model is that you can associate visibility policies with records and metadata. Here we demonstrate how you can add a new metadata entry to a record and make it visible only to a particular group of users. \n",
    "\n",
    "Update the metadata added below, and confirm that the queries now show the updated metadata. Note: When querying over these entities the results will collapse metadata for the same root subject. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_data = {\n",
    "    \"ingest_type\": \"GMetaEntry\",\n",
    "    \"ingest_data\": {\n",
    "        \"subject\": \"globus://%s%s%s\" % (publication_endpoint, share_path, share_file),\n",
    "        \"id\": \"rating\",\n",
    "        \"visible_to\": ['urn:globus:groups:id:%s' % access_group],\n",
    "        \"content\": {\n",
    "            \"rating\": \"good\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "result = search.ingest(search_index, ingest_data)\n",
    "print(\"Documents indexed: %s\" % result['num_documents_ingested'])\n",
    "\n",
    "search_results = search.search(search_index, q='identifier: \"%s\"' % metadata['identifier'], advanced=True)\n",
    "\n",
    "print(\"Count: %s\" % search_results['count'])\n",
    "for i in search_results['gmeta']:\n",
    "    print(\"Subject: %s\" % i['subject'])\n",
    "    print(\"Content: %s\" % i['content'])                                   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
